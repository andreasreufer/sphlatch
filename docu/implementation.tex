%% implementation

\section{SPHLATCH - An implementation}

This section shows an approach to parallelize a Barnes \& Hut tree in an easy way. A few code examples are given in C/C++.

\subsection{Parallizing a particle calculation}
To speed up a particle calculation, it is desirable to be able to let it run in parallel on multiple processors. There exist many different parallel setups, the one considered here consists of multiple processors with strictly local memory not directly accessible by the remote processors. Interprocessor communication is possible through an API like MPI.\\
The computational cost for calculating the derivatives like acceleration due to gravity or some SPH-sum in an SPH calculation, is typically of the same magnitude for every single particle. So a natural way of parallelizing such a calculation is to divide the set of particles used in the calculation into about equally sized subsets. Each subset is then assigned to a process. These subsets can be created by subdividing the space in which all particles lie into \emph{computational domains}, volumes containing the desired subset of particles. For the parallelization method used for the tree, it is necessary to build these computational domains out of volumes with a side length of $l / 2^{d_{cz}}$, where $l$ is the side length of the root node of the octree or in other words the side length of the smallest cube containing all the particles aligned with the coordinate system and $d_{cz}$ the \emph{cost zone depth}. 
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{quadtree50_xy_TPL2.pdf}
\includegraphics[scale=0.3]{quadtree50_TPL2_stage3.pdf}
\caption{A part of the same particle distribution like in figure \ref{fig:2D_BHtree}. The local domain is shaded green and contains 21 particles for which the acceleration has to be calculated. Bordering this zone is the ghost domain shaded grey, for which all particles are known. Beyond that, no particle information is available, only the corresponding parts of the global tree down to top-tree depth are known here. The plot below shows the corresponding tree. Note the filled top-tree nodes without any children nodes, they correspond to the domain where no particle information is available.}
\label{fig:2D_BHtree_costzone}
\end{center}
\end{figure}
Figure \ref{fig:2D_BHtree_costzone} shows one computational domain of such a partition of the same particle distribution like in figure \ref{fig:2D_BHtree}. The $21$ particles calculated in the current computational are called \emph{local particles} and are shown in green, the corresponding domain volumes are shaded green. In this example the cost zone depth is $2$ or the paritioning volumes are obtained by subdividing space twice in each dimension. \\
Local particles may use information about particles in other computational domains. To compute an SPH-sum, usually information about particles within two smoothings lengths is needed. When a local particle is right at the edge of the domain, it needs to know about non-local particles in the adjacent domain. Particles inside non-local volumes sharing an edge with a local volume are called \emph{ghost particles} oder just \emph{ghosts}. Besided having to be copied on to the local process, they cause no computational cost, their information is only used by the local particles.\\

Peano-Hilbert

\subsection{Parallizing a B\&H-tree}
Parallelizing the gravity calculation with a  B\&H-tree is not as straightforward like paralleizing a particle calculation for shprt-range forces, where distant non-local particles just can be omitted. The crucial point is to know, which parts the local particles need to know about the \emph{global tree} generated by all the particles, local and non-local. The tree known to the local particles is called the \emph{local tree} and provides all the necessary information to the local particles. It differs from the \emph{global tree} as such as it does not contain certain sub-trees not needed by the local particles. The sub-trees which can be omitted can be easily identified by the MAC and a worst-case scenario for the distribution of a local and a non-local particle shown in figure \ref{fig:2D_BHtree_costzone}. Assume a particle right at the edge of the computational domain, marked in the figure with a red dot. The shortest distance between this particle and any point in the non-local domain for which no particle information is available, marked in the figure with a red line, is the side length of the cost zone volumes or in other words the distance between the particle any point in the non-local domain $r_{WC}$ is always bigger than this side length
\begin{equation}
r_{WC} \ge \frac{l}{2^{d_{CZ}} } 
\end{equation}
The MAC now tells us how big a cell might be to still be accepted for acceleration calculation, given the worst case distance or in other words, it tells us how deep we have to go in the tree until the multipole approximation is acceptable. Taking the worst case with distance $r_{WC}$ we get the deepest depth we have to go in the tree for a point in the non-local domain, we'll call that the \emph{top-tree depth} $d_{TT}$
\begin{equation}
\frac{l_{cell}}{r_{WC}} = \frac{l}{r_{WC} 2^{d_{TT}}} \ge \theta
\end{equation}
The top-tree depth is down to which the global tree has to be known on every computational domain. Any tree-structure below that depth is either provided by the local or ghost particles, or is not needed locally. Combining the two equations we get a relation between the top-tree depth, the costzone depth and the opening angle from the MAC.
\begin{equation}
2^{d_{CZ} - d_{TT}} \le \theta ~~~~\text{or}~~~~ d_{TT} = d_{CZ} + \uparrow \Big( - \frac{ \log{\theta} }{\log{2} } \Big)
\end{equation}
The first expression tells us down to which opening angle the tree contains enough information for a given costzone and top tree depth. The latter expression gives the needed top tree depth for a given costzone depth and opening angle. The rounding up is because depths are always integer numbers. The example shown in  figure \ref{fig:2D_BHtree_costzone} uses a top-tree depth of 2 and also a costzone depth of 2, so $\theta \ge 1$, in order to use a smaller opening angle we would have to increase the top-tree depth.\\
So we know now which parts of the global tree need to be in the local tree. The next question is how to build the local tree. On no process the global tree can be built, as no process knows about all the particles and also the global tree might be too big to fit into the memory available to a process. The basic idea is to build the local tree with local and ghost particles and then synchronize the \emph{top-tree} which is the global tree starting from the root node down to top-tree depth.
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.35]{quadtree50_TPL2_stage1.pdf}\\ \vspace{1.0cm}
\includegraphics[scale=0.35]{quadtree50_TPL2_stage2a.pdf}\\
\includegraphics[scale=0.35]{quadtree50_TPL2_stage2b.pdf}\\
\includegraphics[scale=0.35]{quadtree50_TPL2_stage3.pdf}\\
\caption{Building stages of the local tree for the computational domain in figure \ref{fig:2D_BHtree_costzone}. The first tree shows the empty top-tree, a full quad-tree with depth 2. Then the local and the ghost particles are inserted leading to the second and third tree. As a final step the top-tree is summed up globally, filling up the 4 previously empty cell nodes. Top-tree cell nodes are coloured blue, normal cell nodes red and particles green. Ghost particles or cells are coloured grey. Non-filled shapes represent empty nodes.}
\label{fig:2D_BHtree_building}
\end{center}
\end{figure}\\

Synchronizing the top-tree is per se non-trivial, as a process does not know which parts of the non-local domain contain particles and therefore need nodes to represent them and which don't. So the easisiest solution is to build a \emph{full} top-tree, this means each node in the top-tree has the maximum possible number of children. In 2D this corresponds to a full quad-tree, in 3D to a ful octree. The problem is now, that for non-uniform particle distributions we get many top-tree nodes which exist, but do not really have any particles below them, which will lead to unnecessary walks along them and vanishing contributions to the acceleration. The calculation does not get incorrect, but the adding up of vanishing terms leads to a higher computational cost. Therefore we introduce an \emph{emptiness} flag to each cell node. A priori a cell nodes is empty. Only when a particle is inserted the cell node becomes unempty or in other words filled. Emptiness is inherited bottom-up from the children to the parent node in a negative way, this means when a node has any non-empty child, it becomes non-empty. This flag can now be queried during tree-walks in order to avoid empty walking along empty sub-trees. So we start building the tree by setting up a full top-tree consisting of empty nodes. The first tree in figure \ref{fig:2D_BHtree_building} shows such an empty quadtree with top-tree depth of two. Every process has now a top-tree of the same structure and therefore we avoid synchronizing different tree structures between the processes.\\

For the next step. the particles locally known to a process, namely the local and ghost particles are added to the local tree, inserting needed cell nodes. After that the multipole moments can be calculated bottom-up from the children to their parents. This is a little bit tricky again, as a particle may be present on different process at the same time: On the process assigned to its computational domain as a local particles and probably to one or more domains as a ghost particle. We have to make sure, that it contributes to the multipole moments of the tree only once. But we also have to make sure, that the ghost particles contribute to the multipole moments of a ghost cell node. An example of such a ghost cell node can be seen in the third and fourth tree in figure \ref{fig:2D_BHtree_building} represented by grey squares. This cell nodes may also be used for the acceleration calculation and therefore have to contain the correct multipole moments. For this reason we introduce a \emph{locality} flag to each node. A local particle has the locality \emph{local}, a ghost particle the locality \emph{remote}. Locality is again inherited bottom-up from the childred to their parent nodes in a positive sense. If one children is local, the parent node is also local. Only when all children are remote, the parent is remote. Top-tree nodes are excluded from this rule and have always the locality local. So when calculating the multipole moments of a node, we apply the rule that only nodes with the same locality contribute to the multipole moments. With that rule we fulfill both requirements mentioned above: Ghost nodes don't contribute to the globally synchronized top-tree, but ghost cell nodes still get their correct multipole moments based on their remote or ghost children.\\

After adding up the multipoles, we can sum up the multipole moments of the top-tree. This is straightforward, as all top-trees have the same structure. The emptiness flag is summed up with an OR-relation, respecting the additive nature of multipole moments addition. After this step we end up with a valid Barnes\& Hut tree containing all the required particles and cells needed to calculate the gravitational acceleration for the local particles.\\

This parallelization method is rather simple, which is one of the advantages of it. Another advantage is, that after the particles and ghosts have been copied to the processes, only one, big additional communication step is required for summing up the multipole moments of the top-tree nodes. When summing up the multipole moments between two processes, all the data can be sent to one process and summed up there. This is bulk communication and therefore ideal for communication networks with high bandwidth. Latency is not important. In practice, the synchronization needs only a few percent of the total time of the gravity calculation.\\

The downside of the method is that for non-uniform particle distribution a lot of unused and empty top-tree nodes have to be created. Although they don't need to be accessed, they use memory and worsen the caching of other tree nodes. The enforcment of cell nodes in the top-tree also causes a deeper tree than without the top-tree, which lead to longer and thereforce slightly slower tree walks.\\

\subsection{Implementing a tree datastructure}
The most common and probably also easiest way to implement a tree datastructure is by using \emph{pointers}. Pointers are a datatype pointing to another datatype, which can be a pointer again. One now defines a node datastructure, which is made up of pointers to others nodes, namely the parent node and the children node. If a child does not exist, corresponding pointer is set to the null pointer. As there may be more than one child, an array is used to store the child pointers with a size corresponding to the maximum number of children. Other information can also be stored in the tree node, for example in the case of a Barnes \& Hut tree the depth and the center coordinates of a cell.
\begin{verbatim}
struct node {
    node* parent;
    node* child[8];    // an octree
    int depth;
    
    // let's store some useful data on this node
    float centerX, centerY, centerZ;
    ...
}
\end{verbatim}

The tree can now be contructed by letting the nodes point to each other. If we don't want to have to access the tree nodes explicitely, it is necessary to define a \emph{cursor}. The cursor is a node pointer and can therefore point to any node in the tree. Data of the nodes can be accessed through the cursor. 
\begin{verbatim}
...
// assume we have the tree: Root -> Child0 -> Grandchild3
node* Cursor;
Cursor = *Root;               // initialize the Cursor
Cursor = Cursor->child[0];    // Cursor now points to Child0
Cursor->centerX = 42.;        // set centerX of Childo to 42
Cursor = Cursor->child[3];    // Cursor now points to Grandchild3
Cursor = Cursor->parent;      // ... to Child0 again
...
\end{verbatim}

Elementary tree operations can now be implemented, like going up to the parent node or going to a child.\\

Tree walks can be implemented be implemented by recursive functions, so called \emph{recursors}:
\begin{verbatim}
void postorderRecursor() {
    // do something
    useful();
    
    // walk existing children
    for (size_t i = 0; i < 8; i++) {        // assume 8 childs
         if ( Cursor->child[i] != NULL ) {  // does the child exist?
             goChild(i);
             postorderRecursor();           // recurse
             goParent();
         }
    }
}
...
goRoot();               // go to the root node
postorderRecursor();    // walk the tree
...
\end{verbatim}

The cursor will point to every tree node once in a post order and every time the function \verb|useful()| is executed. A pre-order tree walk or a walk of a subtree can be implemented in a similar fashion.\\

Each time the recursor calls itself, the current context is laid on to the stack, so the stack-depth is incremented by one. On a real-world computer, the stack has a finite size. When we want to walk a tree with mind-blowing depth, the stack depth will increase the deeper we go in the tree and will possibly overflow, leading to a program crash. So is recursion dangerous for tree walks? In the case of Barnes \& Hut trees it is not.\\

This can be shown with a quick back of a napkin calculation: Let's imagine a worst-case scenario with two particles positioned at almost the same coordinates. When trying to insert the second particle at almost the same place like the first particle, the two come to lie in the same cell octant so that a new children cell has to be created at this octant. Each time this happens, the resolution to distinguish the two particles increase by two or in other words, the the difference in coordinates of the particles has to be half as small in order for the tree not to be able to distinguish the two particles again. Or we can also say, one more bit of the binary representation of the particles coordinates have to match each other. Particle coordinates are usually stored as floating point numbers. A floating point number is usually represented according to the \emph{IEEE 754} standard, with 24 bits precision for a single precision float and 53 bits for a double precision float. So in a worst case, the tree gets a depth of around 24 or 53, a depth which can be handled by the stacks of modern computers. And yet this worst-case is very pathological, as a calculation with such high dynamics easily leads to other problems like over- or underflows. Concluding we can say, that recursion for the B\&H-tree walks is not dangerous.

\subsection{Implementing a B\&H-tree}
The gravitational acceleration calculation with a parallelized B\&H-tree like shown before breaks down to 4 major steps: Building the top-tree, inserting the particles and ghosts, calculating the multipole moments and finally calculating the resulting acceleration on the particles. If the tree is not needed any more, the tree has also to be deleted. For this five steps, there exist easy recursive algorithms.\\

After the root node has been created, 

%top-tree build algorithm
\begin{algorithm}
\caption{top-tree build recursor}
\begin{algorithmic}
\label{alg:buildtoptree}
\IF{ depth $\ge$  top-tree depth}
\STATE{make emtpy cell node}
\FORALL{children}
\STATE{go to child}
\STATE{call top-tree build recursor}
\STATE{go to parent}
\ENDFOR
\ENDIF
\end{algorithmic}
\end{algorithm}

Inserting a particle into the tree can also be done with a recursive function. The idea is to start at the root node and try to insert the particle as a child of the current node. There are three cases possible: The child node does not exist, so the particle can be inserted directly. If there is already a cell node, go there and call the insertion function again. If there is already a particle node, save this particle temporarily and call the insertion function for this particle and the one that has been to be inserted in the first place. At the beginning of the recursor, the current node is set to be non-empty, so that the inserting a particle leaves a path from the root node to the particle of non-empty nodes.
%particle insertion algorithm
\begin{algorithm}
\caption{insert particle $p_{i}$ recursor}
\begin{algorithmic}
\label{alg:insertparticle}
\STATE{$k =$ subVolumeIndex($p_{i}$)}
\STATE{set current node to non-empty}
\IF{child $k$ is not existing }
\STATE{go to child $k$}
\STATE{make particle node, fill it with $p_{i}$}
\STATE{go to parent}
\ELSIF{child $k$ is a cell node}
\STATE{go to child $k$}
\STATE{call insert particle $p_{i}$ recursor}
\STATE{go to parent}
\ELSIF{child $k$ is a particle node}
\STATE{save particle from child $k$ to $p_{j}$}
\STATE{go to child $k$}
\STATE{convert particle node to cell node}
\STATE{call insert particle recursor for $p_{i}$}
\STATE{call insert particle recursor for $p_{j}$}
\STATE{go to parent}
\ENDIF
\end{algorithmic}
\end{algorithm}\\

When the tree has been built, the multipole moments can be calculated by iterating through all non-empty tree nodes in a pre-order fashion, so that the multipole moments of the children of a node are already calculated when the multipoles of the node has to be calculated. The multipoles calculation recursor starts at the root node. After its execution, all cell nodes below the top-tree contain the correct multipole moments, this is also the case for cell nodes containing only ghost nodes. After executing the recursor, the multipoles in the top-tree are added up globally, so that also the top-tree cell nodes contain the correct multipole moments. This is done by summing up between processes pair-wise with a binary tree, so that even for a high number of processes there are not too many communication steps. The total of the multipoles is then distributed again to every process with the same binary in the opposite direction.
%multipole calculation algorithm
\begin{algorithm}
\caption{multipoles calculation recursor}
\begin{algorithmic}
\label{alg:multipolecalc}
\IF{current node is not empty }
\FORALL{existing children}
\STATE{go to child}
\STATE{call multipoles calculation recursor}
\STATE{go to parent}
\IF{ depth $>$ top-tree depth }
\STATE{calculate multipoles from children nodes with the same locality}
\ELSE
\STATE{calculate multipoles from local children nodes}
\ENDIF
\ENDFOR
\ENDIF
\end{algorithmic}
\end{algorithm}
\\

To calculate the acceleration due to gravity for a particle, a recursive tree walk starting from the root node has to be undertaken. The recursion is stopped, when either the MAC is fulfilled or the current node is a particle or empty. In order not to calculate the gravitational interaction of the particle with itself, before calling the algorithm, the particle node is made to look like an empty cell node, so that no interaction is calculated. After the execution of the algorithm, the corresponding node is changed back again to a particle node.
%acceleration algorithm
\begin{algorithm}
\caption{acceleration calculation recursor}
\begin{algorithmic}
\label{alg:calcgravity}
\IF{current node is a particle node}
\STATE{calculate acceleration due to a particle}
\ELSIF{current node is empty}
\STATE{do nothing}
\ELSIF{MAC is fulfilled}
\STATE{calculate acceleration due to a cell}
\ELSE
\FORALL{existing children}
\STATE{go to child}
\STATE{call acceleration calculation recursor}
\STATE{go to parent}
\ENDFOR
\ENDIF
\end{algorithmic}
\end{algorithm}\\

The tree has to be deleted again after the calculation. In order not to disconnect any nodes from the tree, the nodes are deleted in a pre-order fashion with algorithm \ref{alg:treedeletion}.
%tree deletion algorihm
\begin{algorithm}
\caption{tree deletion recursor}
\begin{algorithmic}
\label{alg:treedeletion}
\FORALL{children}
\STATE{go to child}
\STATE{call tree deletion recursor}
\STATE{go to parent}
\STATE{delete current node}
\ENDFOR
\end{algorithmic}
\end{algorithm}
